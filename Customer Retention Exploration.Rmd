---
title: "Grinds Exploration on Client Retention"
author: "Alex Heck, Tom Zwiller, Giulia Neves Monteiro, Aziz Al Mezraani, Madison Bradley"
date: "`r Sys.Date()`"
output: html_document
---

## Importing the relevant packages needed:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(ggthemes)
library("randomForest")
library("rpart")
library("caret")
library("splitstackshape")
library("DMwR")
library("nnet")
library("xgboost")
library("gam")

library(stringr)
```

## Data Preparation

```{r}
# Read it in, but not as a factor
returning_data1 = read.csv("C:\\Users\\Graduate\\Downloads\\sales_2023-12-01_2024-11-01_Grinds Confidential.csv", header = TRUE)

# This removes the net_quantity rows that are less than 0! We kept exactly 0 because that suggests returns
returning_data1 <- returning_data1[returning_data1$net_quantity >= 0, ]

# Cleaning column names and removing the special attributes
colnames(returning_data1) <- make.names(colnames(returning_data1))

Encoding(returning_data1$product_title) <- "UTF-8"

returning_data1$product_title <- iconv(returning_data1$product_title, 
                                      "UTF-8", "UTF-8", sub = "")

returning_data1$product_title <- gsub("^\\W\\s", "", returning_data1$product_title, perl = TRUE)


# Substitute all region names not found in US states to international
returning_data1$billing_region <- ifelse(!(returning_data1$billing_region %in% state.name), "International", returning_data1$billing_region)

# Remove blanks in product_vendor
returning_data1 <- returning_data1[returning_data1$product_vendor != "", ]

# Convert Product Title column to lowercase
returning_data1$product_title <- tolower(returning_data1$product_title)

# Rename
returning_data1$product_title[returning_data1$product_title == 'grinds coffee pouches - vanilla'] <- 'vanilla'

# Rename
returning_data1$product_title[returning_data1$product_title == 'grinds coffee pouches - 3pack variety with double mocha, sweet mint and cinnamon roll'] <- '3 pack variety'

# Rename
returning_data1$product_title[returning_data1$product_title == '3-pack energy sampler'] <- '3 pack energy sampler'

# Rename
returning_data1$product_title[returning_data1$product_title == 'Mocha'] <- 'DoubleMocha'

# Remove dashes
returning_data1$product_title <- gsub("-.*", "", returning_data1$product_title)

# Convert to title case for better readability
returning_data1$product_title <- str_to_title(returning_data1$product_title)

# Remove spaces
returning_data1$product_title <- gsub(" ", "", returning_data1$product_title)

# Remove '100% off'
returning_data1$product_title <- gsub("\\(100%off\\)", "", returning_data1$product_title, ignore.case = TRUE)

# View
head(returning_data1)

# Pull unique titles
unique(returning_data1$product_title)
```
## Data Partitioning 

Doing stratified to ensure there's a more balanced separation between response variables in both training and test sets.
```{r}
library(dplyr)
returning_data <- returning_data1 %>% 
  select(-customer_id, -billing_city, -product_vendor) %>%  # Removing unnecessary variables that will not contribute to the logistic regression development
  mutate(customer_type = ifelse(customer_type == "First-time", 0, 1)) # converting response variable to a binary number 0 or 1
          
split_data <- stratified(returning_data, 
                        group = c("customer_type"), 
                        size = 0.8, bothSets = TRUE)
```

Partitioning data to training and test sets: 
```{r}
train_data <- as.data.frame(na.omit(split_data[[1]]))

test_data <- na.omit(split_data[[2]])
```


## Logsitic Regression
```{r}
fit_1 <- glm(customer_type ~ discounts+returns+billing_region, # Set formula
             family=binomial, # Set logistic regression
             data= train_data) # Set data set
t1 <- summary(fit_1) # Summarize model
t1
```

Analyzing the coefficients to see which are significant:

```{r}
t1$coefficients[which(t1$coefficients[,4] >= 0.01),]
```

## Model Evaluation

```{r}
library(caret)

fit_1_pred <- predict(fit_1, newdata=test_data, type='response')
```

Evaluating the predictive performance:

```{r Q5(c)}
lm_full_acc <- confusionMatrix(factor(ifelse(fit_1_pred>0.5, '1', '0')), as.factor(test_data$customer_type), positive='1')
```

```{r}
# To simplify our comparison, get the results in an easier format:
print(c(lm_full_acc$overall[1], lm_full_acc$byClass[1:2]))
```

The logistic regression doesn't provide great accuracy, having only a 58.5% score. Many variables couldn't be included as well since their coefficient values were so high that they were not allowing the logistic regression to create a sigmoid function and run it's model.

Let's do another model:

## GAM

Convert to factors
```{r}
returning_data1$product_title <- as.factor(returning_data1$product_title)
returning_data1$customer_type <- as.factor(returning_data1$customer_type)
returning_data1$billing_region <- as.factor(returning_data1$billing_region)
```

Adjusting one product title name:
```{r}
# Rename
returning_data1$product_title[returning_data1$product_title == 'mocha'] <- 'doublemocha'
returning_data1$product_title[returning_data1$product_title == 'cherry'] <- 'peach' # putting it with another fruity flavor since there isn't any other that's similar. 
```


Sample:
```{r}
split_data1 <- stratified(returning_data1, 
                        group = c("customer_type"), 
                        size = 0.8, bothSets = TRUE)

train <- as.data.frame(na.omit(split_data1[[1]]))

test <- as.data.frame(na.omit(split_data1[[2]]))
```

Adjusting some predictors and completing the model:
```{r}
# Taxes and discounts as percent of gross_sales
train$taxes_per <- train$taxes/train$gross_sales
test$taxes_per <- test$taxes/test$gross_sales
train$discounts_per <- abs(train$discounts/train$gross_sales)
    train$discounts_per <- as.numeric(gsub("NaN", "100", train$discounts_per))
    train$discounts_per <- as.numeric(gsub("Inf", "0", train$discounts_per))
test$discounts_per <- abs(test$discounts/test$gross_sales)
    test$discounts_per <- as.numeric(gsub("NaN", "100", test$discounts_per))
    test$discounts_per <- as.numeric(gsub("Inf", "0", test$discounts_per))

# Build on train data
gam1 <- gam(customer_type~product_title+billing_region+s(net_quantity)+s(discounts_per)+s(returns)+s(total_sales), family = 'binomial', data = train)

summary(gam1)


#plot(gam1, col='blue')
```

```{r}
# Evaluation on test data
gam1_pred <- predict(gam1, newdata=test, type='response')
gam1_acc <- confusionMatrix(factor(ifelse(gam1_pred>0.5, 'Returning', 'First-time')), test$customer_type, positive='Returning')
print(gam1_acc)
```
The GAM model is showing us a much better accuracy score of 65%, doing a much better job at predicting the customers more likely to return.

